{
    "title": "밑바닥부터 시작하는 딥러닝 ",
    "introduce": "직접 구현하고 움직여보며 익히는 가장 쉬운 딥러닝 입문서\n\n이 책은 라이브러리나 프레임워크에 의존하지 않고, 딥러닝의 핵심을 ‘밑바닥부터’ 직접 만들어보며 즐겁게 배울 수 있는 본격 딥러닝 입문서이다. 술술 읽힐 만큼 쉽게 설명하였고, 역전파처럼 어려운 내용은 ‘계산 그래프’ 기법으로 시각적으로 풀이했다. 무엇보다 작동하는 코드가 있어 직접 돌려보고 요리조리 수정해보면 어려운 이론도 명확하게 이해할 수 있다. 딥러닝에 새롭게 입문하려는 분과 기초를 다시금 정리하고 싶은 현업 연구자와 개발자에게 최고의 책이 될 것이다.",
    "toc": "1장 헬로 파이썬\n1.1 파이썬이란?\n1.2 파이썬 설치하기\n__1.2.1 파이썬 버전\n__1.2.2 사용하는 외부 라이브러리\n__1.2.3 아나콘다 배포판\n1.3 파이썬 인터프리터\n__1.3.1 산술 연산\n__1.3.2 자료형\n__1.3.3 변수\n__1.3.4 리스트\n__1.3.5 딕셔너리\n__1.3.6 bool\n__1.3.7 if 문\n__1.3.8 for 문\n__1.3.9 함수\n1.4 파이썬 스크립트 파일\n__1.4.1 파일로 저장하기\n__1.4.2 클래스\n1.5 넘파이\n__1.5.1 넘파이 가져오기\n__1.5.2 넘파이 배열 생성하기\n__1.5.3 넘파이의 산술 연산\n__1.5.4 넘파이의 N차원 배열\n__1.5.5 브로드캐스트\n__1.5.6 원소 접근\n1.6 matplotlib\n__1.6.1 단순한 그래프 그리기\n__1.6.2 pyplot의 기능\n__1.6.3 이미지 표시하기\n1.7 정리\n\n2장 퍼셉트론\n2.1 퍼셉트론이란?\n2.2 단순한 논리 회로\n__2.2.1 AND 게이트\n__2.2.2 NAND 게이트와 OR 게이트\n2.3 퍼셉트론 구현하기\n__2.3.1 간단한 구현부터\n__2.3.2 가중치와 편향 도입\n__2.3.3 가중치와 편향 구현하기\n2.4 퍼셉트론의 한계\n__2.4.1 도전! XOR 게이트\n__2.4.2 선형과 비선형\n2.5 다층 퍼셉트론이 출동한다면\n__2.5.1 기존 게이트 조합하기\n__2.5.2 XOR 게이트 구현하기\n2.6 NAND에서 컴퓨터까지\n2.7 정리\n\n3장 신경망\n3.1 퍼셉트론에서 신경망으로\n__3.1.1 신경망의 예\n__3.1.2 퍼셉트론 복습\n__3.1.3 활성화 함수의 등장\n3.2 활성화 함수\n__3.2.1 시그모이드 함수\n__3.2.2 계단 함수 구현하기\n__3.2.3 계단 함수의 그래프\n__3.2.4 시그모이드 함수 구현하기\n__3.2.5 시그모이드 함수와 계단 함수 비교\n__3.2.6 비선형 함수\n__3.2.7 ReLU 함수\n3.3 다차원 배열의 계산\n__3.3.1 다차원 배열\n__3.3.2 행렬의 내적\n__3.3.3 신경망의 내적\n3.4 3층 신경망 구현하기\n__3.4.1 표기법 설명\n__3.4.2 각 층의 신호 전달 구현하기\n__3.4.3 구현 정리\n3.5 출력층 설계하기\n__3.5.1 항등 함수와 소프트맥스 함수 구현하기\n__3.5.2 소프트맥스 함수 구현 시 주의점\n__3.5.3 소프트맥스 함수의 특징\n__3.5.4 출력층의 뉴런 수 정하기\n3.6 손글씨 숫자 인식\n__3.6.1 MNIST 데이터셋\n__3.6.2 신경망의 추론 처리\n__3.6.3 배치 처리\n3.7 정리\n\n4장 신경망 학습\n4.1 데이터에서 학습한다!\n__4.1.1 데이터 주도 학습\n__4.1.2 훈련 데이터와 시험 데이터\n4.2 손실 함수\n__4.2.1 평균 제곱 오차\n__4.2.2 교차 엔트로피 오차\n__4.2.3 미니배치 학습\n__4.2.4 (배치용) 교차 엔트로피 오차 구현하기\n__4.2.5 왜 손실 함수를 설정하는가?\n4.3 수치 미분\n__4.3.1 미분\n__4.3.2 수치 미분의 예\n__4.3.3 편미분\n4.4 기울기\n__4.4.1 경사법(경사 하강법)\n__4.4.2 신경망에서의 기울기\n4.5 학습 알고리즘 구현하기\n__4.5.1 2층 신경망 클래스 구현하기\n__4.5.2 미니배치 학습 구현하기\n__4.5.3 시험 데이터로 평가하기\n4.6 정리\n\n5장 오차역전파법\n5.1 계산 그래프\n__5.1.1 계산 그래프로 풀다\n__5.1.2 국소적 계산\n__5.1.3 왜 계산 그래프로 푸는가?\n5.2 연쇄법칙\n__5.2.1 계산 그래프에서의 역전파\n__5.2.2 연쇄법칙이란?\n__5.2.3 연쇄법칙과 계산 그래프\n5.3 역전파\n__5.3.1 덧셈 노드의 역전파\n__5.3.2 곱셈 노드의 역전파\n__5.3.3 사과 쇼핑의 예\n5.4 단순한 계층 구현하기\n__5.4.1 곱셈 계층\n__5.4.2 덧셈 계층\n5.5 활성화 함수 계층 구현하기\n__5.5.1 ReLU 계층\n__5.5.2 Sigmoid 계층\n5.6 Affine/Softmax 계층 구현하기\n__5.6.1 Affine 계층\n__5.6.2 배치용 Affine 계층\n__5.6.3 Softmax-with-Loss 계층\n5.7 오차역전파법 구현하기\n__5.7.1 신경망 학습의 전체 그림\n__5.7.2 오차역전파법을 적용한 신경망 구현하기\n__5.7.3 오차역전파법으로 구한 기울기 검증하기\n__5.7.4 오차역전파법을 사용한 학습 구현하기\n5.8 정리\n\n6장 학습 관련 기술들\n6.1 매개변수 갱신\n__6.1.1 모험가 이야기\n__6.1.2 확률적 경사 하강법(SGD)\n__6.1.3 SGD의 단점\n__6.1.4 모멘텀\n__6.1.5 AdaGrad\n__6.1.6 Adam\n__6.1.7 어느 갱신 방법을 이용할 것인가?\n__6.1.8 MNIST 데이터셋으로 본 갱신 방법 비교\n6.2 가중치의 초깃값\n__6.2.1 초깃값을 0으로 하면?\n__6.2.2 은닉층의 활성화 분포\n__6.2.3 ReLU를 사용할 때의 가중치 초깃값\n__6.2.4 MNIST 데이터셋으로 본 가중치 초깃값 비교\n6.3 배치 정규화\n__6.3.1 배치 정규화 알고리즘\n__6.3.2 배치 정규화의 효과\n6.4 바른 학습을 위해\n__6.4.1 오버피팅\n__6.4.2 가중치 감소\n__6.4.3 드롭아웃\n6.5 적절한 하이퍼파라미터 값 찾기\n__6.5.1 검증 데이터\n__6.5.2 하이퍼파라미터 최적화\n__6.5.3 하이퍼파라미터 최적화 구현하기\n6.6 정리\n\n7장 합성곱 신경망(CNN)\n7.1 전체 구조\n7.2 합성곱 계층\n__7.2.1 완전연결 계층의 문제점\n__7.2.2 합성곱 연산\n__7.2.3 패딩\n__7.2.4 스트라이드\n__7.2.5 3차원 데이터의 합성곱 연산\n__7.2.6 블록으로 생각하기\n__7.2.7 배치 처리\n7.3 풀링 계층\n__7.3.1 풀링 계층의 특징\n7.4 합성곱/풀링 계층 구현하기\n__7.4.1 4차원 배열\n__7.4.2 im2col로 데이터 전개하기\n__7.4.3 합성곱 계층 구현하기\n__7.4.4 풀링 계층 구현하기\n7.5 CNN 구현하기\n7.6 CNN 시각화하기\n__7.6.1 1번째 층의 가중치 시각화하기\n__7.6.2 층 깊이에 따른 추출 정보 변화\n7.7 대표적인 CNN\n__7.7.1 LeNet\n__7.7.2 AlexNet\n7.8 정리\n\n8장 딥러닝\n8.1 더 깊게\n__8.1.1 더 깊은 네트워크로\n__8.1.2 정확도를 더 높이려면\n__8.1.3 깊게 하는 이유\n8.2 딥러닝의 초기 역사\n__8.2.1 이미지넷\n__8.2.2 VGG\n__8.2.3 GoogLeNet\n__8.2.4 ResNet\n8.3 더 빠르게(딥러닝 고속화)\n__8.3.1 풀어야 할 숙제\n__8.3.2 GPU를 활용한 고속화\n__8.3.3 분산 학습\n__8.3.4 연산 정밀도와 비트 줄이기\n8.4 딥러닝의 활용\n__8.4.1 사물 검출\n__8.4.2 분할\n__8.4.3 사진 캡션 생성\n8.5 딥러닝의 미래\n__8.5.1 이미지 스타일(화풍) 변환\n__8.5.2 이미지 생성\n__8.5.3 자율 주행\n__8.5.4 Deep Q-Network(강화학습)\n8.6 정리\n\n부록 A Softmax-with-Loss 계층의 계산 그래프\nA.1 순전파\nA.2 역전파\nA.3 정리\n참고문헌"
}